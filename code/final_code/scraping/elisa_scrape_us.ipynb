{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is function used to scrape U.S. Glassdoor job listings for positions 'junior data scientist' and 'senior scientist'. The scraping function is adapted from [this helpful blog post by Ã–mer Sakarya](https://towardsdatascience.com/selenium-tutorial-scraping-glassdoor-com-in-10-minutes-3d0915c6d905).\n",
    "\n",
    "The 2 dataframes (one for Jr. data scientist, one for Sr. data scientist) are merged to result in a data frame with 1805 job listings (rows) and 12 columns (attributes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_listings(job_title, country, num_scrape, debug = True):\n",
    "    '''\n",
    "    arguments \n",
    "        - job_title - str, delimiter '-', job position searched e.g. 'junior-data-scientist'\n",
    "        - country - str, country searched e.g. 'us'\n",
    "        - num_scrape - number of job listings to scrape\n",
    "        \n",
    "    return - pandas dataframe of Glassdoor job listing attributes, size =  `num_job` by ?? (# attributes)\n",
    "    '''\n",
    "    \n",
    "    # FIX: use here::here() for driver path\n",
    "    # initialize Chrome driver saved in local path\n",
    "    driver = webdriver.Chrome(executable_path = \"../../../chromedriver\")\n",
    "    \n",
    "    # go to glassdoor search results\n",
    "    url = 'https://www.glassdoor.ca/Job/' + country + '-' + job_title + '-jobs-SRCH_IL.0,2_IN1_KO3,24.htm'\n",
    "    driver.get(url)\n",
    "    (driver.page_source).encode('utf-8','ignore') # gets rid of special char. in job description \n",
    "    \n",
    "    jobs = []\n",
    "    \n",
    "    # scrape listing until total of `num_scrape` reached\n",
    "    while len(jobs) < num_scrape:\n",
    "        \n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            driver.find_element_by_class_name(\"selected\").click()\n",
    "        except ElementClickInterceptedException:\n",
    "            pass\n",
    "        \n",
    "        time.sleep(1)\n",
    "        \n",
    "        # close sign-in popup / Glassdoor salary popup\n",
    "        try: \n",
    "            driver.find_element_by_class_name(\"modal_closeIcon\").click()\n",
    "        except NoSuchElementException: \n",
    "            pass\n",
    "        \n",
    "        time.sleep(0.7)\n",
    "\n",
    "        # go through listings\n",
    "        listings = driver.find_elements_by_class_name(\"jl\")\n",
    "        \n",
    "        for l in listings: \n",
    "            \n",
    "            # print scraping progress\n",
    "            print(\"Scraped:{} \".format(\"\" + str(len(jobs)) + \"/\" + str(num_scrape)))\n",
    "            \n",
    "            # terminate condition: if target <= current scraped\n",
    "            if num_scrape <= len(jobs):\n",
    "                break\n",
    "                \n",
    "            # click a listing     \n",
    "            l.click()  \n",
    "            time.sleep(1.1)\n",
    "            \n",
    "            # special case for handling popup\n",
    "            if job_title == 'senior-data-scientist':\n",
    "                if len(jobs) == 0 or len(jobs) == 1 or len(jobs) == 4 or len(jobs) == 3:\n",
    "                    try: \n",
    "                        driver.find_element_by_class_name(\"modal_closeIcon\").click()\n",
    "                    except NoSuchElementException: \n",
    "                        pass\n",
    "            \n",
    "            ## start scraping\n",
    "            # get the key info assumed to be on EVERY company's listing\n",
    "            keyInfo_done = False\n",
    "            while not keyInfo_done:\n",
    "                try:\n",
    "                    job_position = driver.find_element_by_xpath('.//div[contains(@class, \"title\")]').text\n",
    "                    company = driver.find_element_by_xpath('.//div[@class=\"employerName\"]').text\n",
    "                    location = driver.find_element_by_xpath('.//div[@class=\"location\"]').text\n",
    "                    description = driver.find_element_by_xpath('.//div[@class=\"jobDescriptionContent desc\"]').text\n",
    "                    keyInfo_done = True\n",
    "                \n",
    "                # if any key info above is missing in current listing\n",
    "                except: \n",
    "                    time.sleep(3)\n",
    "            \n",
    "            time.sleep(1.9)\n",
    "            \n",
    "            # get glassdoor salary estimate\n",
    "            try:\n",
    "                salary_est = driver.find_element_by_xpath('.//div[@class=\"salary\"]/span[@class=\"css-1uyte9r css-hca4ks e1wijj242\"]').text\n",
    "            except NoSuchElementException:\n",
    "                salary_est = 'NF' # if not found\n",
    "            \n",
    "           # get company rating (out of 5)\n",
    "            try:\n",
    "                rate = driver.find_element_by_xpath('.//span[@class=\"rating\"]').text \n",
    "            except NoSuchElementException:\n",
    "                rate = 'NF'\n",
    "                \n",
    "            time.sleep(1.3)\n",
    "            \n",
    "            # debugging pt.1\n",
    "            if debug:\n",
    "                print(\"Job Title: {}\".format(job_position))\n",
    "                print(\"Salary Estimate: {}\".format(salary_est))\n",
    "                print(\"Job Description: {}\".format(description[:200]))\n",
    "                print(\"Rating: {}\".format(rate))\n",
    "                print(\"Company Name: {}\".format(company))\n",
    "                print(\"Location: {}\".format(location))\n",
    "            \n",
    "            # special case for handling popup\n",
    "            if job_title == 'senior-data-scientist':\n",
    "                if len(jobs) == 0 or len(jobs) == 1 or len(jobs) == 4 or len(jobs) == 3:\n",
    "                    try: \n",
    "                        driver.find_element_by_class_name(\"modal_closeIcon\").click()\n",
    "                    except NoSuchElementException: \n",
    "                        pass\n",
    "                    \n",
    "            # click company tab & scrape\n",
    "            try:\n",
    "                driver.find_element_by_xpath('.//div[@class=\"tab\" and @data-tab-type=\"overview\"]').click() # click tab\n",
    "                \n",
    "                time.sleep(0.6)\n",
    "                # company size \n",
    "                try:\n",
    "                    size = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Size\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    size = 'NF'\n",
    "\n",
    "                # year founded\n",
    "                try:\n",
    "                    yr_found = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Founded\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    yr_found = 'NF'\n",
    "                \n",
    "                time.sleep(1.6)\n",
    "                \n",
    "                # company type (e.g. public, private)\n",
    "                try:\n",
    "                    company_type = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Type\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    company_type = 'NF'\n",
    "                # industry\n",
    "                try:\n",
    "                    industry = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Industry\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    industry = 'NF'\n",
    "                \n",
    "                time.sleep(0.9)\n",
    "                \n",
    "                # sector\n",
    "                try:\n",
    "                    sector = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Sector\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    sector = 'NF'\n",
    "                # annual revenue\n",
    "                try:\n",
    "                    revenue = driver.find_element_by_xpath('.//div[@class=\"infoEntity\"]//label[text()=\"Revenue\"]//following-sibling::*').text\n",
    "                except NoSuchElementException:\n",
    "                    revenue = 'NF'\n",
    "                time.sleep(2.3)\n",
    "\n",
    "            # if no company tab exists\n",
    "            except NoSuchElementException:  \n",
    "                size = 'NF'\n",
    "                yr_found = 'NF'\n",
    "                company_type = 'NF'\n",
    "                industry = 'NF'\n",
    "                sector = 'NF'\n",
    "                revenue = 'NF'\n",
    "            \n",
    "            # debugging pt. 2\n",
    "            if debug:\n",
    "                print(\"Size: {}\".format(size))\n",
    "                print(\"Founded: {}\".format(yr_found))\n",
    "                print(\"Type of Ownership: {}\".format(company_type))\n",
    "                print(\"Industry: {}\".format(industry))\n",
    "                print(\"Sector: {}\".format(sector))\n",
    "                print(\"Revenue: {}\".format(revenue))\n",
    "                print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "                \n",
    "            # append current listing\n",
    "            jobs.append({\"Position\" : job_position, \n",
    "                         \"Description\" : description,\n",
    "                         \"Company\" : company.split('\\n')[0],\n",
    "                         \"Location\" : location,\n",
    "                         \"Glassdoor Salary Estimate\" : salary_est,\n",
    "                         \"Rating\" : rate, \n",
    "                         \"Size\" : size,\n",
    "                         \"Revenue\" : revenue,\n",
    "                         \"Type\" : company_type, \n",
    "                         \"Year Founded\" : yr_found, \n",
    "                         \"Industry\" : industry,\n",
    "                         \"Sector\" : sector,\n",
    "                        })\n",
    "\n",
    "            time.sleep(1.2) # pause before click next listing\n",
    "            \n",
    "        # Click on `next page` after all listings on current page scraped\n",
    "        try:\n",
    "            driver.find_element_by_xpath('.//li[@class=\"next\"]//a').click()\n",
    "        except NoSuchElementException: # reach end of all listings available, but <`num_scrape`\n",
    "            print(\"Stopped scraping. Target {} listings, actually scraped {} listings.\".format(num_scrape, len(jobs)))\n",
    "            break\n",
    "        \n",
    "        # terminate webdriver\n",
    "           # driver.quit() # use only if web driver opens > 2 browsing windows when program runs\n",
    "        #driver.close()\n",
    "\n",
    "    # dataframe of listings\n",
    "    return pd.DataFrame(jobs, columns=jobs[0].keys())  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# junior data scientist in U.S.  | 30 pages of listings, each with 30 or 32 listings (do 30 * 32 = 960 max. to be safe)\n",
    "df1 = get_listings('junior-data-scientist', 'us', 960, debug = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# senior data scientist in U.S. \n",
    "df2 = get_listings('senior-data-scientist', 'us', 960, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 2 dfs\n",
    "# write to csv (U.S.)\n",
    "df = pd.concat([df1, df2], axis = 0)\n",
    "df.to_csv('elisa_us.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
